<div class='block' >

    <h2>2016</h2>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Kaeding16_FDN">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Kaeding16_FDN.pdf">Fine-tuning Deep Neural Networks in Continuous Learning Scenarios</a>.</span>
          <br />
          <span class=bibauthor>Christoph Käding and Erik Rodner and Alexander Freytag and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>ACCV Workshop on Interpretation and Visualization of Deep Neural Nets (ACCV-WS).</span>
          
          <span class=bibyear>2016.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Kaeding16_FDN" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Kaeding16_FDN.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="http://www.interpretable-ml.org/accv2016workshop/"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/supplementary/.pdf"><img src="bibworldfiles/supplementary.png" alt="supplementary" title="supplementary" target="_blank"/></a>
        
        
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Rodner16_FRN">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Rodner16_FRN.pdf">Fine-grained Recognition in the Noisy Wild: Sensitivity Analysis of Convolutional Neural Networks Approaches</a>.</span>
          <br />
          <span class=bibauthor>Erik Rodner and Marcel Simon and Bob Fisher and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>British Machine Vision Conference (BMVC).</span>
          
          <span class=bibyear>2016.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Rodner16_FRN" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Rodner16_FRN.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        
        
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/supplementary/.pdf"><img src="bibworldfiles/supplementary.png" alt="supplementary" title="supplementary" target="_blank"/></a>
        
        
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Amthor16_IDD">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Amthor16_IDD.pdf">Impatient DNNs - Deep Neural Networks with Dynamic Time Budgets</a>.</span>
          <br />
          <span class=bibauthor>Manuel Amthor and Erik Rodner and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>British Machine Vision Conference (BMVC).</span>
          
          <span class=bibyear>2016.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Amthor16_IDD" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Amthor16_IDD.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        
        
        
        
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Brust16:EQL">
          
        
        </td>
        <td style='vertical-align:middle'>
        
          <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Brust16:EQL.pdf">Neither Quick Nor Proper -- Evaluation of QuickProp for Learning Deep Neural Networks</a>.</span>
          <br />
          <span class=bibauthor>Clemens-Alexander Brust and Sven Sickert and Marcel Simon and Erik Rodner and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>arXiv preprint arXiv:1606.04333.</span>
          
          
          <span class=bibyear>2016.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Brust16:EQL" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Brust16:EQL.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="https://arxiv.org/abs/1606.04333"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Brust16:EQL');">more ...</span>
        <div class="bibabstract" id="Brust16:EQL">
          <br />
          <div class='vspace'></div>
          Abstract: Neural networks and especially convolutional neural networks are of great  interest in current computer vision research. However, many techniques, extensions,  and modifications have been published in the past, which are not yet used by  current approaches. In this paper, we study the application of a method called  QuickProp for training of deep neural networks. In particular, we apply QuickProp  during learning and testing of fully convolutional networks for the task of  semantic segmentation. We compare QuickProp empirically with gradient descent,  which is the current standard method. Experiments suggest that QuickProp can not  compete with standard gradient descent techniques for complex computer vision  tasks like semantic segmentation.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Kaeding16_ACE">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Kaeding16_ACE.pdf">Active and Continuous Exploration with Deep Neural Networks and Expected Model Output Changes</a>.</span>
          <br />
          <span class=bibauthor>Christoph Käding and Erik Rodner and Alexander Freytag and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>NIPS Workshop on Continual Learning and Deep Networks (NIPS-WS).</span>
          
          <span class=bibyear>2016.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Kaeding16_ACE" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Kaeding16_ACE.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="https://sites.google.com/site/cldlnips2016/"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    

    <h2>2015</h2>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Simon15:NAC">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Simon15:NAC.pdf">Neural Activation Constellations: Unsupervised Part Model Discovery with Convolutional Networks</a>.</span>
          <br />
          <span class=bibauthor>Marcel Simon and Erik Rodner.</span>
          <br />
          <span class=bibvenue>International Conference on Computer Vision (ICCV).</span>
          
          <span class=bibyear>2015.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Simon15:NAC" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Simon15:NAC.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="http://arxiv.org/abs/1504.08289"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Simon15:NAC');">more ...</span>
        <div class="bibabstract" id="Simon15:NAC">
          <br />
          <div class='vspace'></div>
          Abstract: Part models of object categories are essential for challenging recognition tasks, where differences in categories are subtle and only reflected in appearances of small parts of the object. We present an approach that is able to learn part models in a completely unsupervised manner, without part annotations and even without given bounding boxes during learning. The key idea is to find constellations of neural activation patterns computed using convolutional neural networks. In our experiments, we outperform existing approaches for fine-grained recognition on the CUB200-2011, Oxford PETS, and Oxford Flowers dataset in case no part or bounding box annotations are available and achieve state-of-the-art performance for the Stanford Dog dataset. We also show the benefits of neural constellation models as a data augmentation technique for fine-tuning. Furthermore, our paper unites the areas of generic and fine-grained classification, since our approach is suitable for both scenarios.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Brust15:CPN">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Brust15:CPN.pdf">Convolutional Patch Networks with Spatial Prior for Road Detection and Urban Scene Understanding</a>.</span>
          <br />
          <span class=bibauthor>Clemens-Alexander Brust and Sven Sickert and Marcel Simon and Erik Rodner and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>International Conference on Computer Vision Theory and Applications (VISAPP).</span>
          
          <span class=bibpages>510-517.</span>
          
          <span class=bibyear>2015.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Brust15:CPN" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Brust15:CPN.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="http://arxiv.org/abs/1502.06344"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        <a href="http://cvjena.github.io/cn24/"><img src="bibworldfiles/code-link.png" alt="code" title="code" target="_blank"/></a>
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Brust15:CPN');">more ...</span>
        <div class="bibabstract" id="Brust15:CPN">
          <br />
          <div class='vspace'></div>
          Abstract: Classifying single image patches is important in many different applications, such as road detection or scene understanding.  In this paper, we present convolutional patch networks, which are convolutional networks learned to distinguish different image patches and  which can be used for pixel-wise labeling.  We also show how to incorporate spatial information of the patch as an input to the network, which allows for learning spatial priors for  certain categories jointly with an appearance model.  In particular, we focus on road detection and urban scene understanding, two application areas where we are able to achieve state-of-the-art  results on  the KITTI as well as on the LabelMeFacade dataset.  Furthermore, our paper offers a guideline for people working in the area and desperately wandering through all the painstaking details that  render training CNs on image patches extremely difficult.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Brust15:ECP">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Brust15:ECP.pdf">Efficient Convolutional Patch Networks for Scene Understanding</a>.</span>
          <br />
          <span class=bibauthor>Clemens-Alexander Brust and Sven Sickert and Marcel Simon and Erik Rodner and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>CVPR Workshop on Scene Understanding (CVPR-WS).</span>
          
          <span class=bibyear>2015.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Brust15:ECP" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Brust15:ECP.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        
        <a href="http://cvjena.github.io/cn24/"><img src="bibworldfiles/code-link.png" alt="code" title="code" target="_blank"/></a>
        
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/presentation/Brust15:ECP.pdf"><img src="bibworldfiles/presentation.png" alt="presentation" title="presentation" target="_blank"/></a>
        
        
        
        <span class="abstractlink" onClick="abstractclick('Brust15:ECP');">more ...</span>
        <div class="bibabstract" id="Brust15:ECP">
          <br />
          <div class='vspace'></div>
          Abstract: In this paper, we present convolutional patch networks, which are convolutional (neural) networks (CNN) learned to distinguish  different image patches and which can be used for pixel-wise labeling. We show how to easily learn spatial priors for certain categories jointly  with their appearance. Experiments for urban scene understanding demonstrate state-of-the-art results on the LabelMeFacade dataset. Our approach  is implemented as a new CNN framework especially designed for semantic segmentation with fully-convolutional architectures.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Rodner15:FRD">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Rodner15:FRD.pdf">Fine-grained Recognition Datasets for Biodiversity Analysis</a>.</span>
          <br />
          <span class=bibauthor>Erik Rodner and Marcel Simon and Gunnar Brehm and Stephanie Pietsch and J. Wolfgang Wägele and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>CVPR Workshop on Fine-grained Visual Classification (CVPR-WS).</span>
          
          <span class=bibyear>2015.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Rodner15:FRD" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Rodner15:FRD.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="http://www.inf-cv.uni-jena.de/fgvcbiodiv"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Guadarrama15:UOD">
          
        
        </td>
        <td style='vertical-align:middle'>
        
          <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Guadarrama15:UOD.pdf">Understanding Object Descriptions in Robotics by Open-vocabulary Object Retrieval and Detection</a>.</span>
          <br />
          <span class=bibauthor>Sergio Guadarrama and Erik Rodner and Kate Saenko and Trevor Darrell.</span>
          <br />
          <span class=bibvenue>International Journal of Robotics Research (IJRR).</span>
          
          <span class=bibpages>35(1-3):</span>
          
          
          <span class=bibpages>265-280.</span>
          
          <span class=bibyear>2015.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Guadarrama15:UOD" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Guadarrama15:UOD.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="http://ijr.sagepub.com/content/35/1-3/265.abstract"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    

    <h2>2014</h2>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Guadarrama14:OOR">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Guadarrama14:OOR.pdf">Open-vocabulary Object Retrieval</a>.</span>
          <br />
          <span class=bibauthor>Sergio Guadarrama and Erik Rodner and Kate Saenko and Ning Zhang  and Ryan Farrell and Jeff Donahue and Trevor Darrell.</span>
          <br />
          <span class=bibvenue>Robotics Science and Systems (RSS).</span>
          
          <span class=bibpages>41, ISBN 978-0-9923747-0-9.</span>
          
          <span class=bibyear>2014.</span>
        
	
		<span class=awardnote> Awarded with an AAAI invited talk</span>
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Guadarrama14:OOR" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Guadarrama14:OOR.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="http://openvoc.berkeleyvision.org"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Guadarrama14:OOR');">more ...</span>
        <div class="bibabstract" id="Guadarrama14:OOR">
          <br />
          <div class='vspace'></div>
          Abstract: In this paper, we address the problem of retrieving objects based  on open-vocabulary natural language queries: Given a phrase describing  a specific object, e.g., the corn flakes box, the task is to find  the best match in a set of images containing candidate objects. When  naming objects, humans tend to use natural language with rich semantics,  including basic-level categories, fine-grained categories, and instance-level  concepts such as brand names. Existing approaches to large-scale  object recognition fail in this scenario, as they expect queries  that map directly to a fixed set of pre-trained visual categories,  e.g. ImageNet synset tags. We address this limitation by introducing  a novel object retrieval method. Given a candidate object image,  we first map it to a set of words that are likely to describe it,  using several learned image-to-text projections. We also propose  a method for handling open-vocabularies, i.e., words not contained  in the training data. We then compare the natural language query  to the sets of words predicted for each candidate and select the  best match. Our method can combine category- and instance-level semantics  in a common representation. We present extensive experimental results  on several datasets using both instance-level and category-level  matching and show that our approach can accurately retrieve objects  based on extremely varied open-vocabulary queries. The source code  of our approach will be publicly available together with pre-trained  models and could be directly used for robotics applications.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Freytag14:ESP">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Freytag14:ESP.pdf">Exemplar-specific Patch Features for Fine-grained Recognition</a>.</span>
          <br />
          <span class=bibauthor>Alexander Freytag and Erik Rodner and Trevor Darrell and Joachim  Denzler.</span>
          <br />
          <span class=bibvenue>German Conference on Pattern Recognition (GCPR).</span>
          
          <span class=bibpages>144-156.</span>
          
          <span class=bibyear>2014.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Freytag14:ESP" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Freytag14:ESP.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        
        <a href="https://github.com/cvjena/patchDiscovery"><img src="bibworldfiles/code-link.png" alt="code" title="code" target="_blank"/></a>
        
        
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/supplementary/.pdf"><img src="bibworldfiles/supplementary.png" alt="supplementary" title="supplementary" target="_blank"/></a>
        
        
        <span class="abstractlink" onClick="abstractclick('Freytag14:ESP');">more ...</span>
        <div class="bibabstract" id="Freytag14:ESP">
          <br />
          <div class='vspace'></div>
          Abstract: In this paper, we present a new approach for fine-grained recognition or subordinate categorization,  tasks where an algorithm needs to reliably differentiate between visually similar categories, e.g. different bird species.  While previous approaches aim at learning a single generic representation and models with increasing complexity,  we propose an orthogonal approach that learns patch representations specifically tailored to every single test exemplar.  Since we query a constant number of images similar to a given test image,  we obtain very compact features and avoid large-scale training with all classes and examples.  Our learned mid-level features are build on shape and color detectors estimated from discovered patches reflecting small highly discriminative structures in the queried images.  We evaluate our approach for fine-grained recognition on the CUB-2011 birds dataset and show that high recognition rates can be obtained by model combination.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Simon14:PDD">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Simon14:PDD.pdf">Part Detector Discovery in Deep Convolutional Neural Networks</a>.</span>
          <br />
          <span class=bibauthor>Marcel Simon and Erik Rodner and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>Asian Conference on Computer Vision (ACCV).</span>
          
          <span class=bibpages>162-177.</span>
          
          <span class=bibyear>2014.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Simon14:PDD" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Simon14:PDD.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        
        <a href="https://github.com/cvjena/PartDetectorDisovery"><img src="bibworldfiles/code-link.png" alt="code" title="code" target="_blank"/></a>
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Simon14:PDD');">more ...</span>
        <div class="bibabstract" id="Simon14:PDD">
          <br />
          <div class='vspace'></div>
          Abstract: Current fine-grained classification approaches often rely  on a robust localization of object parts to extract  localized feature representations suitable for discrimination.  However, part localization is a  challenging task due to the large variation of appearance and pose.  In this paper, we show how pre-trained convolutional neural networks  can be used for robust and efficient object part discovery and localization without the  necessity to actually train the network on the current dataset. Our approach called  part detector discovery  (PDD)  is based on analyzing the gradient maps of the network outputs and finding  activation centers spatially related to annotated semantic parts or bounding boxes.  This allows us not just to obtain excellent performance on the CUB200-2011 dataset,  but in contrast to previous approaches also to perform detection and bird classification jointly  without requiring a given bounding box annotation during testing and ground-truth parts during training.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Simon14:PLE">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Simon14:PLE.pdf">Part Localization by Exploiting Deep Convolutional Networks</a>.</span>
          <br />
          <span class=bibauthor>Marcel Simon and Erik Rodner and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>ECCV Workshop on Parts and Attributes (ECCV-WS).</span>
          
          <span class=bibyear>2014.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Simon14:PLE" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Simon14:PLE.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="https://filebox.ece.vt.edu/~parikh/PnA2014/"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Freytag14:BFF">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Freytag14:BFF.pdf">Birds of a Feather Flock Together - Local Learning of Mid-level Representations  for Fine-grained Recognition</a>.</span>
          <br />
          <span class=bibauthor>Alexander Freytag and Erik Rodner and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>ECCV Workshop on Parts and Attributes (ECCV-WS).</span>
          
          <span class=bibyear>2014.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Freytag14:BFF" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Freytag14:BFF.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="https://filebox.ece.vt.edu/~parikh/PnA2014/"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        <a href="https://github.com/cvjena/patchDiscovery"><img src="bibworldfiles/code-link.png" alt="code" title="code" target="_blank"/></a>
        
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/presentation/Freytag14:BFF.pdf"><img src="bibworldfiles/presentation.png" alt="presentation" title="presentation" target="_blank"/></a>
        
        
        
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    

</div>