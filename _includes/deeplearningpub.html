<div class='block' >

    <h2>2015</h2>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          Convolutional Patch Networks with Spatial Prior for Road Detection and Urban Scene Understanding.</span>
          <br />
          <span class=bibauthor>Clemens-Alexander Brust and Sven Sickert and Marcel Simon and Erik Rodner and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>Proceedings of the International Conference on Computer Vision Theory and Applications (VISAPP).</span>
          
          <span class=bibyear>2015.</span>
        
		
		
		<span class=note> accepted for publication</span>
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Brust15:CPN" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Brust15:CPN');">more ...</span>
        <div class="bibabstract" id="Brust15:CPN">
          <br />
          <div class='vspace'></div>
          Abstract: Classifying single image patches is important in many different applications, such as road detection or scene understanding.  In this paper, we present convolutional patch networks, which are convolutional networks learned to distinguish different image patches and  which can be used for pixel-wise labeling.  We also show how to incorporate spatial information of the patch as an input to the network, which allows for learning spatial priors for  certain categories jointly with an appearance model.  In particular, we focus on road detection and urban scene understanding, two application areas where we are able to achieve state-of-the-art  results on  the KITTI as well as on the LabelMeFacade dataset.  Furthermore, our paper offers a guideline for people working in the area and desperately wandering through all the painstaking details that  render training CNs on image patches extremely difficult.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    

    <h2>2014</h2>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Guadarrama14:OOR">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Guadarrama14:OOR.pdf">Open-vocabulary Object Retrieval</a>.</span>
          <br />
          <span class=bibauthor>Sergio Guadarrama and Erik Rodner and Kate Saenko and Ning Zhang  and Ryan Farrell and Jeff Donahue and Trevor Darrell.</span>
          <br />
          <span class=bibvenue>Robotics Science and Systems (RSS).</span>
          
          <span class=bibyear>2014.</span>
        
		
		
		<span class=note> accepted for publication, ISBN 978-0-9923747-0-9</span>
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Guadarrama14:OOR" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Guadarrama14:OOR.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="http://openvoc.berkeleyvision.org"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Guadarrama14:OOR');">more ...</span>
        <div class="bibabstract" id="Guadarrama14:OOR">
          <br />
          <div class='vspace'></div>
          Abstract: In this paper, we address the problem of retrieving objects based  on open-vocabulary natural language queries: Given a phrase describing  a specific object, e.g., the corn flakes box, the task is to find  the best match in a set of images containing candidate objects. When  naming objects, humans tend to use natural language with rich semantics,  including basic-level categories, fine-grained categories, and instance-level  concepts such as brand names. Existing approaches to large-scale  object recognition fail in this scenario, as they expect queries  that map directly to a fixed set of pre-trained visual categories,  e.g. ImageNet synset tags. We address this limitation by introducing  a novel object retrieval method. Given a candidate object image,  we first map it to a set of words that are likely to describe it,  using several learned image-to-text projections. We also propose  a method for handling open-vocabularies, i.e., words not contained  in the training data. We then compare the natural language query  to the sets of words predicted for each candidate and select the  best match. Our method can combine category- and instance-level semantics  in a common representation. We present extensive experimental results  on several datasets using both instance-level and category-level  matching and show that our approach can accurately retrieve objects  based on extremely varied open-vocabulary queries. The source code  of our approach will be publicly available together with pre-trained  models and could be directly used for robotics applications.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Freytag14:ESP">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Freytag14:ESP.pdf">Exemplar-specific Patch Features for Fine-grained Recognition</a>.</span>
          <br />
          <span class=bibauthor>Alexander Freytag and Erik Rodner and Trevor Darrell and Joachim  Denzler.</span>
          <br />
          <span class=bibvenue>German Conference on Pattern Recognition (GCPR).</span>
          
          <span class=bibpages>144-156.</span>
          
          <span class=bibyear>2014.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Freytag14:ESP" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Freytag14:ESP.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        
        <a href="https://github.com/cvjena/patchDiscovery"><img src="bibworldfiles/code-link.png" alt="code" title="code" target="_blank"/></a>
        
        
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/supplementary/.pdf"><img src="bibworldfiles/supplementary.png" alt="supplementary" title="supplementary" target="_blank"/></a>
        
        
        <span class="abstractlink" onClick="abstractclick('Freytag14:ESP');">more ...</span>
        <div class="bibabstract" id="Freytag14:ESP">
          <br />
          <div class='vspace'></div>
          Abstract: In this paper, we present a new approach for fine-grained recognition or subordinate categorization,  tasks where an algorithm needs to reliably differentiate between visually similar categories, e.g. different bird species.  While previous approaches aim at learning a single generic representation and models with increasing complexity,  we propose an orthogonal approach that learns patch representations specifically tailored to every single test exemplar.  Since we query a constant number of images similar to a given test image,  we obtain very compact features and avoid large-scale training with all classes and examples.  Our learned mid-level features are build on shape and color detectors estimated from discovered patches reflecting small highly discriminative structures in the queried images.  We evaluate our approach for fine-grained recognition on the CUB-2011 birds dataset and show that high recognition rates can be obtained by model combination.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Simon14:PDD">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Simon14:PDD.pdf">Part Detector Discovery in Deep Convolutional Neural Networks</a>.</span>
          <br />
          <span class=bibauthor>Marcel Simon and Erik Rodner and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>Asian Conference on Computer Vision (ACCV).</span>
          
          <span class=bibyear>2014.</span>
        
		
		
		<span class=note> accepted for publication</span>
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Simon14:PDD" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Simon14:PDD.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        
        <a href="https://github.com/cvjena/PartDetectorDisovery"><img src="bibworldfiles/code-link.png" alt="code" title="code" target="_blank"/></a>
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Simon14:PDD');">more ...</span>
        <div class="bibabstract" id="Simon14:PDD">
          <br />
          <div class='vspace'></div>
          Abstract: Current fine-grained classification approaches often rely  on a robust localization of object parts to extract  localized feature representations suitable for discrimination.  However, part localization is a  challenging task due to the large variation of appearance and pose.  In this paper, we show how pre-trained convolutional neural networks  can be used for robust and efficient object part discovery and localization without the  necessity to actually train the network on the current dataset. Our approach called  part detector discovery  (PDD)  is based on analyzing the gradient maps of the network outputs and finding  activation centers spatially related to annotated semantic parts or bounding boxes.  This allows us not just to obtain excellent performance on the CUB200-2011 dataset,  but in contrast to previous approaches also to perform detection and bird classification jointly  without requiring a given bounding box annotation during testing and ground-truth parts during training.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Simon14:PLE">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Simon14:PLE.pdf">Part Localization by Exploiting Deep Convolutional Networks</a>.</span>
          <br />
          <span class=bibauthor>Marcel Simon and Erik Rodner and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>ECCV Workshop on Parts and Attributes.</span>
          
          <span class=bibyear>2014.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Simon14:PLE" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Simon14:PLE.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="https://filebox.ece.vt.edu/~parikh/PnA2014/"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Freytag14:BFF">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Freytag14:BFF.pdf">Birds of a Feather Flock Together - Local Learning of Mid-level Representations  for Fine-grained Recognition</a>.</span>
          <br />
          <span class=bibauthor>Alexander Freytag and Erik Rodner and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>ECCV Workshop on Parts and Attributes.</span>
          
          <span class=bibyear>2014.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Freytag14:BFF" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Freytag14:BFF.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="https://filebox.ece.vt.edu/~parikh/PnA2014/"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        <a href="https://github.com/cvjena/patchDiscovery"><img src="bibworldfiles/code-link.png" alt="code" title="code" target="_blank"/></a>
        
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/presentation/Freytag14:BFF.pdf"><img src="bibworldfiles/presentation.png" alt="presentation" title="presentation" target="_blank"/></a>
        
        
        
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    

</div>