<div class='block' >

    <h2>2020</h2>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://141.45.92.125:5000/teaser/Simon19:Implicit">
          
        
        </td>
        <td style='vertical-align:middle'>
        
          <span class=bibtitle>
          <a href="http://141.45.92.125:5000/pdf/Simon19:Implicit.pdf">The whole is more than its parts? From explicit to implicit pose normalization</a>.</span>
          <br />
          <span class=bibauthor>Marcel Simon and Erik Rodner and Trevor Darell and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>IEEE Transactions on Pattern Analysis and Machine Intelligence.</span>
          
          <span class=bibpages>42(3):</span>
          
          
          <span class=bibpages>749-763.</span>
          
          <span class=bibyear>2020.</span>
        
		
		
	
        <a href="http://141.45.92.125:5000/bib/Simon19:Implicit" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://141.45.92.125:5000/pdf/Simon19:Implicit.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Simon19:Implicit');">more ...</span>
        <div class="bibabstract" id="Simon19:Implicit">
          <br />
          <div class='vspace'></div>
          Abstract: Fine-grained classification describes the automated recognition of visually similar object categories like birds species. Previous works were usually based on explicit pose normalization, i.e., the detection and description of object parts. However, recent models based on a final global average or bilinear pooling have achieved a comparable accuracy without this concept. In this paper, we analyze the advantages of these approaches over generic CNNs and explicit pose normalization approaches. We also show how they can achieve an implicit normalization of the object pose. A novel visualization technique called activation flow is introduced to investigate limitations in pose handling in traditional CNNs like AlexNet and VGG. Afterward, we present and compare the explicit pose normalization approach neural activation constellations and a generalized framework for the final global average and bilinear pooling called -pooling. We observe that the latter often achieves a higher accuracy improving common CNN models by up to 22.9%, but lacks the interpretability of the explicit approaches. We present a visualization approach for understanding and analyzing predictions of the model to address this issue. Furthermore, we show that our approaches for fine-grained recognition are beneficial for other fields like action recognition.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    

    <h2>2019</h2>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://141.45.92.125:5000/teaser/Barz18:MDI">
          
        
        </td>
        <td style='vertical-align:middle'>
        
          <span class=bibtitle>
          <a href="http://141.45.92.125:5000/pdf/Barz18:MDI.pdf">Detecting Regions of Maximal Divergence for Spatio-Temporal Anomaly Detection</a>.</span>
          <br />
          <span class=bibauthor>Björn Barz and Erik Rodner and Yanira Guanche Garcia and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>IEEE Transactions on Pattern Analysis and Machine Intelligence.</span>
          
          <span class=bibpages>41(5):</span>
          
          
          <span class=bibpages>1088-1101.</span>
          
          <span class=bibyear>2019.</span>
        
		
		
		<span class=note> (Pre-print published in 2018.)</span>
		
	
        <a href="http://141.45.92.125:5000/bib/Barz18:MDI" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://141.45.92.125:5000/pdf/Barz18:MDI.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="https://cvjena.github.io/libmaxdiv/"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        <a href="https://github.com/cvjena/libmaxdiv"><img src="bibworldfiles/code-link.png" alt="code" title="code" target="_blank"/></a>
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Barz18:MDI');">more ...</span>
        <div class="bibabstract" id="Barz18:MDI">
          <br />
          <div class='vspace'></div>
          Abstract: Automatic detection of anomalies in space- and time-varying measurements is an important tool in several fields, e.g., fraud detection, climate analysis, or healthcare monitoring. We present an algorithm for detecting anomalous regions in multivariate spatio-temporal time-series, which allows for spotting the interesting parts in large amounts of data, including video and text data. In opposition to existing techniques for detecting isolated anomalous data points, we propose the "Maximally Divergent Intervals" (MDI) framework for unsupervised detection of coherent spatial regions and time intervals characterized by a high Kullback-Leibler divergence compared with all other data given. In this regard, we define an unbiased Kullback-Leibler divergence that allows for ranking regions of different size and show how to enable the algorithm to run on large-scale data sets in reasonable time using an interval proposal technique. Experiments on both synthetic and real data from various domains, such as climate analysis, video surveillance, and text forensics, demonstrate that our method is widely applicable and a valuable tool for finding interesting events in different types of data.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://141.45.92.125:5000/teaser/Rodner19:Fully">
          
        
        </td>
        <td style='vertical-align:middle'>
        
          <span class=bibtitle>
          Fully Convolutional Networks in Multimodal Nonlinear Microscopy Images for Automated Detection of Head and Neck Carcinoma: A Pilot Study.</span>
          <br />
          <span class=bibauthor>Erik Rodner and Thomas Bocklitz and Ferdinand von Eggeling and Günther Ernst and Olga Chernavskaia and Jürgen Popp and Joachim Denzler and Orlando Guntinas-Lichius.</span>
          <br />
          <span class=bibvenue>Head and Neck.</span>
          
          <span class=bibpages>41(1):</span>
          
          
          <span class=bibpages>116-121.</span>
          
          <span class=bibyear>2019.</span>
        
		
		
	
        <a href="http://141.45.92.125:5000/bib/Rodner19:Fully" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        
        <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/hed.25489"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Rodner19:Fully');">more ...</span>
        <div class="bibabstract" id="Rodner19:Fully">
          <br />
          <div class='vspace'></div>
          Abstract: A fully convolutional neural networks (FCN)-based automated image analysis algorithm to discriminate between head and neck cancer and noncancerous epithelium based on nonlinear microscopic images was developed. Head and neck cancer sections were used for standard histopathology and co-registered with multimodal images from the same sections using the combination of coherent anti-Stokes Raman scattering, two-photon excited fluorescence, and second harmonic generation microscopy. The images analyzed with semantic segmentation using a FCN for four classes: cancer, normal epithelium, background, and other tissue types. A total of 114 images of 12 patients were analyzed. Using a patch score aggregation, the average recognition rate and an overall recognition rate or the four classes were 88.9\% and 86.7\%, respectively. A total of 113seconds were needed to process a whole-slice image in the dataset. Multimodal nonlinear microscopy in combination with automated image analysis using FCN seems to be a promising technique for objective differentiation between head and neck cancer and noncancerous epithelium.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    

    <h2>2018</h2>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://141.45.92.125:5000/teaser/Kaeding18_ALR">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://141.45.92.125:5000/pdf/Kaeding18_ALR.pdf">Active Learning for Regression Tasks with Expected Model Output Changes</a>.</span>
          <br />
          <span class=bibauthor>Christoph Käding and Erik Rodner and Alexander Freytag and Oliver Mothes and Björn Barz and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>British Machine Vision Conference (BMVC).</span>
          
          <span class=bibyear>2018.</span>
        
		
		
	
        <a href="http://141.45.92.125:5000/bib/Kaeding18_ALR" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://141.45.92.125:5000/pdf/Kaeding18_ALR.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        
        <a href="http://triton.inf-cv.uni-jena.de/LifelongLearning/gpEMOCreg"><img src="bibworldfiles/code-link.png" alt="code" title="code" target="_blank"/></a>
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Kaeding18_ALR');">more ...</span>
        <div class="bibabstract" id="Kaeding18_ALR">
          <br />
          <div class='vspace'></div>
          Abstract: Annotated training data is the enabler for supervised learning.  While recording data at large scale is possible in some application domains,  collecting reliable annotations is time-consuming, costly, and often a project's bottleneck.  Active learning aims at reducing the annotation effort.  While this field has been studied extensively for classification tasks, it has received less attention for regression problems  although the annotation cost is often even higher.  We aim at closing this gap and propose an active learning approach to enable regression applications.  To address continuous outputs, we build on Gaussian process models -- an established tool to tackle even non-linear regression problems.  For active learning, we extend the expected model output change (EMOC) framework to continuous label spaces and show that the involved marginalizations can be solved in closed-form.  This mitigates one of the major drawbacks of the EMOC principle.  We empirically analyze our approach in a variety of application scenarios.  In summary, we observe that our approach can efficiently guide the annotation process and leads to better models in shorter time and at lower costs.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://141.45.92.125:5000/teaser/Qaiser18_HIS">
          
        
        </td>
        <td style='vertical-align:middle'>
        
          <span class=bibtitle>
          HER2 challenge contest: a detailed assessment of automated HER2 scoring algorithms in whole slide images of breast cancer tissues.</span>
          <br />
          <span class=bibauthor>Talha Qaiser and Abhik Mukherjee and Chaitanya Reddy PB and Sai D Munugoti and Vamsi Tallam and Tomi Pitkaho and Taina Lehtimki and Thomas Naughton and Matt Berseth and Anbal Pedraza and Ramakrishnan Mukundan and Matthew Smith and Abhir Bhalerao and Erik Rodner and Marcel Simon and Joachim Denzler and Chao-Hui Huang and Gloria Bueno and David Snead and Ian O Ellis and Mohammad Ilyas and Nasir Rajpoot.</span>
          <br />
          <span class=bibvenue>Histopathology.</span>
          
          <span class=bibpages>72(2):</span>
          
          
          <span class=bibpages>227-238.</span>
          
          <span class=bibyear>2018.</span>
        
		
		
	
        <a href="http://141.45.92.125:5000/bib/Qaiser18_HIS" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        
        <a href="http://dx.doi.org/10.1111/his.13333"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Qaiser18_HIS');">more ...</span>
        <div class="bibabstract" id="Qaiser18_HIS">
          <br />
          <div class='vspace'></div>
          Abstract: Aims  Evaluating expression of the human epidermal growth factor receptor 2 (HER2) by visual examination of immunohistochemistry (IHC) on invasive breast cancer (BCa) is a key part of the diagnostic assessment of BCa due to its recognized importance as a predictive and prognostic marker in clinical practice. However, visual scoring of HER2 is subjective, and consequently prone to interobserver variability. Given the prognostic and therapeutic implications of HER2 scoring, a more objective method is required. In this paper, we report on a recent automated HER2 scoring contest, held in conjunction with the annual PathSoc meeting held in Nottingham in June 2016, aimed at systematically comparing and advancing the state-of-the-art artificial intelligence (AI)-based automated methods for HER2 scoring.  Methods and results  The contest data set comprised digitized whole slide images (WSI) of sections from 86 cases of invasive breast carcinoma stained with both haematoxylin and eosin (H&E) and IHC for HER2. The contesting algorithms predicted scores of the IHC slides automatically for an unseen subset of the data set and the predicted scores were compared with the ground truth (a consensus score from at least two experts). We also report on a simple Man versus Machine contest for the scoring of HER2 and show that the automated methods could beat the pathology experts on this contest data set.  Conclusions  This paper presents a benchmark for comparing the performance of automated algorithms for scoring of HER2. It also demonstrates the enormous potential of automated algorithms in assisting the pathologist with objective IHC scoring.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    

    <h2>2017</h2>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://141.45.92.125:5000/teaser/aubreville2017automatic">
          
        
        </td>
        <td style='vertical-align:middle'>
        
          <span class=bibtitle>
          Automatic Classification of Cancerous Tissue in Laserendomicroscopy Images of the Oral Cavity using Deep Learning.</span>
          <br />
          <span class=bibauthor>Marc Aubreville and Christian Knipfer and Nicolai Oetter and Christian Jaremenko and Erik Rodner and Joachim Denzler and Christopher Bohr and Helmut Neumann and Florian Stelzle and Andreas Maier.</span>
          <br />
          <span class=bibvenue>Scientific Reports.</span>
          
          <span class=bibpages>7(1):</span>
          
          
          <span class=bibpages>41598-017.</span>
          
          <span class=bibyear>2017.</span>
        
		
		
	
        <a href="http://141.45.92.125:5000/bib/aubreville2017automatic" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        
        <a href="https://www5.informatik.uni-erlangen.de/Forschung/Publikationen/2017/Aubreville17-ACO.pdf"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://141.45.92.125:5000/teaser/Rodner16_LGP">
          
        
        </td>
        <td style='vertical-align:middle'>
        
          <span class=bibtitle>
          <a href="http://141.45.92.125:5000/pdf/Rodner16_LGP.pdf">Large-Scale Gaussian Process Inference with Generalized Histogram Intersection Kernels for Visual Recognition Tasks</a>.</span>
          <br />
          <span class=bibauthor>Erik Rodner and Alexander Freytag and Paul Bodesheim and Björn Fröhlich and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>International Journal of Computer Vision (IJCV).</span>
          
          <span class=bibpages>121(2):</span>
          
          
          <span class=bibpages>253-280.</span>
          
          <span class=bibyear>2017.</span>
        
		
		
	
        <a href="http://141.45.92.125:5000/bib/Rodner16_LGP" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://141.45.92.125:5000/pdf/Rodner16_LGP.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="http://dx.doi.org/10.1007/s11263-016-0929-y"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://141.45.92.125:5000/teaser/Barz17_FLP">
          
        
        </td>
        <td style='vertical-align:middle'>
        
          <span class=bibtitle>
          Fast Learning and Prediction for Object Detection using Whitened CNN Features.</span>
          <br />
          <span class=bibauthor>Björn Barz and Erik Rodner and Christoph Käding and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>arXiv preprint arXiv:1704.02930.</span>
          
          
          <span class=bibyear>2017.</span>
        
		
		
	
        <a href="http://141.45.92.125:5000/bib/Barz17_FLP" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        
        <a href="https://arxiv.org/abs/1704.02930"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://141.45.92.125:5000/teaser/Simon17_GOP">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://141.45.92.125:5000/pdf/Simon17_GOP.pdf">Generalized orderless pooling performs implicit salient matching</a>.</span>
          <br />
          <span class=bibauthor>Marcel Simon and Yang Gao and Trevor Darrell and Joachim Denzler and Erik Rodner.</span>
          <br />
          <span class=bibvenue>International Conference on Computer Vision (ICCV).</span>
          
          <span class=bibpages>4970-4979.</span>
          
          <span class=bibyear>2017.</span>
        
		
		
	
        <a href="http://141.45.92.125:5000/bib/Simon17_GOP" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://141.45.92.125:5000/pdf/Simon17_GOP.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        
        
        
        
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    

    <h2>2016</h2>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://141.45.92.125:5000/teaser/Kaeding16_FDN">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://141.45.92.125:5000/pdf/Kaeding16_FDN.pdf">Fine-tuning Deep Neural Networks in Continuous Learning Scenarios</a>.</span>
          <br />
          <span class=bibauthor>Christoph Käding and Erik Rodner and Alexander Freytag and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>ACCV Workshop on Interpretation and Visualization of Deep Neural Nets (ACCV-WS).</span>
          
          <span class=bibyear>2016.</span>
        
		
		
	
        <a href="http://141.45.92.125:5000/bib/Kaeding16_FDN" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://141.45.92.125:5000/pdf/Kaeding16_FDN.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="http://www.interpretable-ml.org/accv2016workshop/"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        <a href="http://141.45.92.125:5000/supplementary/.pdf"><img src="bibworldfiles/supplementary.png" alt="supplementary" title="supplementary" target="_blank"/></a>
        
        
        <span class="abstractlink" onClick="abstractclick('Kaeding16_FDN');">more ...</span>
        <div class="bibabstract" id="Kaeding16_FDN">
          <br />
          <div class='vspace'></div>
          Abstract: The revival of deep neural networks and the availability of ImageNet  laid the foundation for recent success in highly complex recognition tasks. However,  ImageNet does not cover all visual concepts of all possible application scenarios.  Hence, application experts still record new data constantly and expect the  data to be used upon its availability. In this paper, we follow this observation  and apply the classical concept of fine-tuning deep neural networks to scenarios  where data from known or completely new classes is continuously added.  Besides a straightforward realization of continuous fine-tuning, we empirically  analyze how computational burdens of training can be further reduced. Finally,  we visualize how the networks attention maps evolve over time which allows for  visually investigating what the network learned during continuous fine-tuning.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://141.45.92.125:5000/teaser/Rodner16_FRN">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://141.45.92.125:5000/pdf/Rodner16_FRN.pdf">Fine-grained Recognition in the Noisy Wild: Sensitivity Analysis of Convolutional Neural Networks Approaches</a>.</span>
          <br />
          <span class=bibauthor>Erik Rodner and Marcel Simon and Bob Fisher and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>British Machine Vision Conference (BMVC).</span>
          
          <span class=bibyear>2016.</span>
        
		
		
	
        <a href="http://141.45.92.125:5000/bib/Rodner16_FRN" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://141.45.92.125:5000/pdf/Rodner16_FRN.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        
        
        
        <a href="http://141.45.92.125:5000/supplementary/.pdf"><img src="bibworldfiles/supplementary.png" alt="supplementary" title="supplementary" target="_blank"/></a>
        
        
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://141.45.92.125:5000/teaser/Amthor16_IDD">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://141.45.92.125:5000/pdf/Amthor16_IDD.pdf">Impatient DNNs - Deep Neural Networks with Dynamic Time Budgets</a>.</span>
          <br />
          <span class=bibauthor>Manuel Amthor and Erik Rodner and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>British Machine Vision Conference (BMVC).</span>
          
          <span class=bibyear>2016.</span>
        
		
		
	
        <a href="http://141.45.92.125:5000/bib/Amthor16_IDD" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://141.45.92.125:5000/pdf/Amthor16_IDD.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        
        
        
        
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://141.45.92.125:5000/teaser/Brust16:EQL">
          
        
        </td>
        <td style='vertical-align:middle'>
        
          <span class=bibtitle>
          <a href="http://141.45.92.125:5000/pdf/Brust16:EQL.pdf">Neither Quick Nor Proper -- Evaluation of QuickProp for Learning Deep Neural Networks</a>.</span>
          <br />
          <span class=bibauthor>Clemens-Alexander Brust and Sven Sickert and Marcel Simon and Erik Rodner and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>arXiv preprint arXiv:1606.04333.</span>
          
          
          <span class=bibyear>2016.</span>
        
		
		
	
        <a href="http://141.45.92.125:5000/bib/Brust16:EQL" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://141.45.92.125:5000/pdf/Brust16:EQL.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="https://arxiv.org/abs/1606.04333"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Brust16:EQL');">more ...</span>
        <div class="bibabstract" id="Brust16:EQL">
          <br />
          <div class='vspace'></div>
          Abstract: Neural networks and especially convolutional neural networks are of great  interest in current computer vision research. However, many techniques, extensions,  and modifications have been published in the past, which are not yet used by  current approaches. In this paper, we study the application of a method called  QuickProp for training of deep neural networks. In particular, we apply QuickProp  during learning and testing of fully convolutional networks for the task of  semantic segmentation. We compare QuickProp empirically with gradient descent,  which is the current standard method. Experiments suggest that QuickProp can not  compete with standard gradient descent techniques for complex computer vision  tasks like semantic segmentation.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://141.45.92.125:5000/teaser/Rodner16:MDI">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://141.45.92.125:5000/pdf/Rodner16:MDI.pdf">Maximally Divergent Intervals for Anomaly Detection</a>.</span>
          <br />
          <span class=bibauthor>Erik Rodner and Björn Barz and Yanira Guanche and Milan Flach and Miguel Mahecha and Paul Bodesheim and Markus Reichstein and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>ICML Workshop on Anomaly Detection (ICML-WS).</span>
          
          <span class=bibyear>2016.</span>
        
	
		<span class=awardnote> Best Paper Award</span>
	
        <a href="http://141.45.92.125:5000/bib/Rodner16:MDI" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://141.45.92.125:5000/pdf/Rodner16:MDI.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        
        <a href="https://cvjena.github.io/libmaxdiv/"><img src="bibworldfiles/code-link.png" alt="code" title="code" target="_blank"/></a>
        
        
        
        
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://141.45.92.125:5000/teaser/Kaeding16_LAA">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://141.45.92.125:5000/pdf/Kaeding16_LAA.pdf">Large-scale Active Learning with Approximated Expected Model Output Changes</a>.</span>
          <br />
          <span class=bibauthor>Christoph Käding and Alexander Freytag and Erik Rodner and Andrea Perino and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>German Conference on Pattern Recognition (GCPR).</span>
          
          <span class=bibpages>179-191.</span>
          
          <span class=bibyear>2016.</span>
        
		
		
	
        <a href="http://141.45.92.125:5000/bib/Kaeding16_LAA" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://141.45.92.125:5000/pdf/Kaeding16_LAA.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="https://link.springer.com/chapter/10.1007/978-3-319-45886-1_15"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        <a href="http://triton.inf-cv.uni-jena.de/LifelongLearning/gpEMOC"><img src="bibworldfiles/code-link.png" alt="code" title="code" target="_blank"/></a>
        
        
        
        <a href="http://141.45.92.125:5000/supplementary/.pdf"><img src="bibworldfiles/supplementary.png" alt="supplementary" title="supplementary" target="_blank"/></a>
        
        
        <span class="abstractlink" onClick="abstractclick('Kaeding16_LAA');">more ...</span>
        <div class="bibabstract" id="Kaeding16_LAA">
          <br />
          <div class='vspace'></div>
          Abstract: Incremental learning of visual concepts is one step towards reaching human capabilities beyond closed-world assumptions. Besides recent progress, it remains one of the fundamental challenges in computer vision and machine learning. Along that path, techniques are needed which allow for actively selecting informative examples from a huge pool of unlabeled images to be annotated by application experts. Whereas a manifold of active learning techniques exists, they commonly suffer from one of two drawbacks: (i) either they do not work reliably on challenging real-world data or (ii) they are kernel-based and not scalable with the magnitudes of data current vision applications need to deal with. Therefore, we present an active learning and discovery approach which can deal with huge collections of unlabeled real-world data. Our approach is based on the expected model output change principle and overcomes previous scalability issues. We present experiments on the large-scale MS-COCO dataset and on a dataset provided by biodiversity researchers. Obtained results reveal that our technique clearly improves accuracy after just a few annotations. At the same time, it outperforms previous active learning approaches in academic and real-world scenarios.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://141.45.92.125:5000/teaser/Freytag16_CFW">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://141.45.92.125:5000/pdf/Freytag16_CFW.pdf">Chimpanzee Faces in the Wild: Log-Euclidean CNNs for Predicting Identities and Attributes of Primates</a>.</span>
          <br />
          <span class=bibauthor>Alexander Freytag and Erik Rodner and Marcel Simon and Alexander Loos and Hjalmar Kühl and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>German Conference on Pattern Recognition (GCPR).</span>
          
          <span class=bibpages>51-63.</span>
          
          <span class=bibyear>2016.</span>
        
		
		
	
        <a href="http://141.45.92.125:5000/bib/Freytag16_CFW" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://141.45.92.125:5000/pdf/Freytag16_CFW.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="https://link.springer.com/chapter/10.1007/978-3-319-45886-1_5"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        <a href="http://141.45.92.125:5000/supplementary/.pdf"><img src="bibworldfiles/supplementary.png" alt="supplementary" title="supplementary" target="_blank"/></a>
        
        
        <span class="abstractlink" onClick="abstractclick('Freytag16_CFW');">more ...</span>
        <div class="bibabstract" id="Freytag16_CFW">
          <br />
          <div class='vspace'></div>
          Abstract: In this paper, we investigate how to predict attributes of chimpanzees such as identity, age, age group, and gender. We build on convolutional neural networks, which lead to significantly superior results compared with previous state-of-the-art on hand-crafted recognition pipelines. In addition, we show how to further increase discrimination abilities of CNN activations by the Log-Euclidean framework on top of bilinear pooling. We finally introduce two curated datasets consisting of chimpanzee faces with detailed meta-information to stimulate further research. Our results can serve as the foundation for automated large-scale animal monitoring and analysis.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://141.45.92.125:5000/teaser/Denzler16_CNNA">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://141.45.92.125:5000/pdf/Denzler16_CNNA.pdf">Convolutional Neural Networks as a Computational Model for the Underlying Processes of Aesthetics Perception</a>.</span>
          <br />
          <span class=bibauthor>Joachim Denzler and Erik Rodner and Marcel Simon.</span>
          <br />
          <span class=bibvenue>ECCV Workshop on Computer Vision for Art Analysis.</span>
          
          <span class=bibyear>2016.</span>
        
		
		
	
        <a href="http://141.45.92.125:5000/bib/Denzler16_CNNA" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://141.45.92.125:5000/pdf/Denzler16_CNNA.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="https://www.springerprofessional.de/convolutional-neural-networks-as-a-computational-model-for-the-u/10711224"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://141.45.92.125:5000/teaser/Kaeding16_ACE">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://141.45.92.125:5000/pdf/Kaeding16_ACE.pdf">Active and Continuous Exploration with Deep Neural Networks and Expected Model Output Changes</a>.</span>
          <br />
          <span class=bibauthor>Christoph Käding and Erik Rodner and Alexander Freytag and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>NIPS Workshop on Continual Learning and Deep Networks (NIPS-WS).</span>
          
          <span class=bibyear>2016.</span>
        
		
		
	
        <a href="http://141.45.92.125:5000/bib/Kaeding16_ACE" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://141.45.92.125:5000/pdf/Kaeding16_ACE.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="https://sites.google.com/site/cldlnips2016/"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Kaeding16_ACE');">more ...</span>
        <div class="bibabstract" id="Kaeding16_ACE">
          <br />
          <div class='vspace'></div>
          Abstract: The demands on visual recognition systems do not end with the complexity offered  by current large-scale image datasets, such as ImageNet. In consequence, we need  curious and continuously learning algorithms that actively acquire knowledge about  semantic concepts which are present in available unlabeled data. As a step towards  this goal, we show how to perform continuous active learning and exploration,  where an algorithm actively selects relevant batches of unlabeled examples for  annotation. These examples could either belong to already known or to yet undiscovered  classes. Our algorithm is based on a new generalization of the Expected  Model Output Change principle for deep architectures and is especially tailored to  deep neural networks. Furthermore, we show easy-to-implement approximations  that yield efficient techniques for active selection. Empirical experiments show that  our method outperforms currently used heuristics.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    

</div>