<div class='block' >

    <h2>2015</h2>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Bodesheim15:LND">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          Local Novelty Detection in Multi-class Recognition Problems.</span>
          <br />
          <span class=bibauthor>Paul Bodesheim and Alexander Freytag and Erik Rodner and Joachim  Denzler.</span>
          <br />
          <span class=bibvenue>IEEE Winter Conference on Applications of Computer  Vision (WACV).</span>
          
          <span class=bibyear>2015.</span>
        
		
		
		<span class=note> accepted for publication</span>
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Bodesheim15:LND" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        
        
        
        
        
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    

    <h2>2014</h2>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Haase14:ITL">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Haase14:ITL.pdf">Instance-weighted Transfer Learning of Active Appearance Models</a>.</span>
          <br />
          <span class=bibauthor>Daniel Haase and Erik Rodner and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</span>
          
          <span class=bibpages>1426-1433.</span>
          
          <span class=bibyear>2014.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Haase14:ITL" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Haase14:ITL.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Haase14:ITL');">more ...</span>
        <div class="bibabstract" id="Haase14:ITL">
          <br />
          <div class='vspace'></div>
          Abstract: There has been a lot of work on face modeling, analysis, and landmark  detection, with Active Appearance Models being one of the most successful  techniques. A major drawback of these models is the large number  of detailed annotated training examples needed for learning. Therefore,  we present a transfer learning method that is able to learn from  related training data using an instance-weighted transfer technique.  Our method is derived using a generalization of importance sampling  and in contrast to previous work we explicitly try to tackle the  transfer already during learning instead of adapting the fitting  process. In our studied application of face landmark detection, we  efficiently transfer facial expressions from other human individuals  and are thus able to learn a precise face Active Appearance Model  only from neutral faces of a single individual. Our approach is evaluated  on two common face datasets and outperforms previous transfer methods.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Guadarrama14:OOR">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Guadarrama14:OOR.pdf">Open-vocabulary Object Retrieval</a>.</span>
          <br />
          <span class=bibauthor>Sergio Guadarrama and Erik Rodner and Kate Saenko and Ning Zhang  and Ryan Farrell and Jeff Donahue and Trevor Darrell.</span>
          <br />
          <span class=bibvenue>Robotics Science and Systems (RSS).</span>
          
          <span class=bibyear>2014.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Guadarrama14:OOR" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Guadarrama14:OOR.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="http://openvoc.berkeleyvision.org"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Guadarrama14:OOR');">more ...</span>
        <div class="bibabstract" id="Guadarrama14:OOR">
          <br />
          <div class='vspace'></div>
          Abstract: In this paper, we address the problem of retrieving objects based  on open-vocabulary natural language queries: Given a phrase describing  a specific object, e.g., the corn flakes box, the task is to find  the best match in a set of images containing candidate objects. When  naming objects, humans tend to use natural language with rich semantics,  including basic-level categories, fine-grained categories, and instance-level  concepts such as brand names. Existing approaches to large-scale  object recognition fail in this scenario, as they expect queries  that map directly to a fixed set of pre-trained visual categories,  e.g. ImageNet synset tags. We address this limitation by introducing  a novel object retrieval method. Given a candidate object image,  we first map it to a set of words that are likely to describe it,  using several learned image-to-text projections. We also propose  a method for handling open-vocabularies, i.e., words not contained  in the training data. We then compare the natural language query  to the sets of words predicted for each candidate and select the  best match. Our method can combine category- and instance-level semantics  in a common representation. We present extensive experimental results  on several datasets using both instance-level and category-level  matching and show that our approach can accurately retrieve objects  based on extremely varied open-vocabulary queries. The source code  of our approach will be publicly available together with pre-trained  models and could be directly used for robotics applications.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Barz14:ART">
          
        
        </td>
        <td style='vertical-align:middle'>
        
          <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Barz14:ART.pdf">ARTOS -- Adaptive Real-Time Object Detection System</a>.</span>
          <br />
          <span class=bibauthor>Bj√∂rn Barz and Erik Rodner and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>arXiv preprint arXiv:1407.2721.</span>
          
          
          <span class=bibyear>2014.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Barz14:ART" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Barz14:ART.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="http://arxiv.org/abs/1407.2721"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        <a href="http://cvjena.github.io/artos/"><img src="bibworldfiles/code-link.png" alt="code" title="code" target="_blank"/></a>
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Barz14:ART');">more ...</span>
        <div class="bibabstract" id="Barz14:ART">
          <br />
          <div class='vspace'></div>
          Abstract: ARTOS is all about creating, tuning, and applying object detection  models with just a few clicks. In particular, ARTOS facilitates learning  of models for visual object detection by eliminating the burden of  having to collect and annotate a large set of positive and negative  samples manually and in addition it implements a fast learning technique  to reduce the time needed for the learning step. A clean and friendly  GUI guides the user through the process of model creation, adaptation  of learned models to different domains using in-situ images, and  object detection on both offline images and images from a video stream.  A library written in C++ provides the main functionality of ARTOS  with a C-style procedural interface, so that it can be easily integrated  with any other project.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Suesse14:BV">
          
        
        </td>
        <td style='vertical-align:middle'>
        
          <span class=bibtitle>Bildverarbeitung und Objekterkennung: Computer Vision in Industrie  und Medizin</span> 
          <span class=bibauthor>Herbert S√º√üe and Erik Rodner.</span>
          (<span class=bibyear>2014</span>)
        
		
		
		<span class=note> Neues umfangreiches Lehrbuch im Bereich Bildverarbeitung und maschinelles  Lernen</span>
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Suesse14:BV" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        
        <a href="http://www.dbvbuch.de"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Suesse14:BV');">more ...</span>
        <div class="bibabstract" id="Suesse14:BV">
          <br />
          <div class='vspace'></div>
          Abstract: Dieses Buch erlaeutert, wie Informationen automatisch aus Bildern  extrahiert werden. Mit dieser sehr aktuellen Frage beschaeftigt sich  das Buch mittels eines Streifzuges durch die Bildverarbeitung. Dabei  werden sowohl die mathematischen Grundlagen vieler Verfahren der  2D- und 3D Bildanalyse vermittelt als auch deren Nutzen anhand von  Problemstellungen aus vielen Bereichen (Medizin, industrielle Bildverarbeitung,  Objekterkennung) erlaeutert. Das Buch eignet sich sowohl fuer Studierende  der Informatik, Mathematik und Ingenieurwissenschaften als auch fuer  Anwender aus der industriellen Bildverarbeitung.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Hoffman14:ACI">
          
        
        </td>
        <td style='vertical-align:middle'>
        
          <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Hoffman14:ACI.pdf">Asymmetric and Category Invariant Feature Transformations for Domain  Adaptation</a>.</span>
          <br />
          <span class=bibauthor>Judy Hoffman and Erik Rodner and Jeff Donahue and Brian Kulis and  Kate Saenko.</span>
          <br />
          <span class=bibvenue>International Journal of Computer Vision (IJCV).</span>
          
          <span class=bibpages>109(1-2):</span>
          
          
          <span class=bibpages>28-41.</span>
          
          <span class=bibyear>2014.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Hoffman14:ACI" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Hoffman14:ACI.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="http://link.springer.com/article/10.1007/s11263-014-0719-3"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Hoffman14:ACI');">more ...</span>
        <div class="bibabstract" id="Hoffman14:ACI">
          <br />
          <div class='vspace'></div>
          Abstract: We address the problem of visual domain adaptation for transferring  object models from one dataset or visual domain to another. We introduce  a unified flexible model for both supervised and semi-supervised  learning that allows us to learn transformations between domains.  Additionally, we present two instantiations of the model, one for  general feature adaptation/alignment, and one specifically designed  for classification. First, we show how to extend metric learning  methods for domain adaptation, allowing for learning metrics independent  of the domain shift and the final classifier used. Furthermore, we  go beyond classical metric learning by extending the method to asymmetric,  category independent transformations. Our framework can adapt features  even when the target domain does not have any labeled examples for  some categories, and when the target and source features have different  dimensions. Finally, we develop a joint learning framework for adaptive  classifiers, which outperforms competing methods in terms of multi-class  accuracy and scalability. We demonstrate the ability of our approach  to adapt object recognition models under a variety of situations,  such as differing imaging conditions, feature types, and codebooks.  The experiments show its strong performance compared to previous  approaches and its applicability to large-scale scenarios.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Freytag14:ESP">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Freytag14:ESP.pdf">Exemplar-specific Patch Features for Fine-grained Recognition</a>.</span>
          <br />
          <span class=bibauthor>Alexander Freytag and Erik Rodner and Trevor Darrell and Joachim  Denzler.</span>
          <br />
          <span class=bibvenue>German Conference on Pattern Recognition (GCPR).</span>
          
          <span class=bibpages>144-156.</span>
          
          <span class=bibyear>2014.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Freytag14:ESP" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Freytag14:ESP.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        
        <a href="https://github.com/cvjena/patchDiscovery"><img src="bibworldfiles/code-link.png" alt="code" title="code" target="_blank"/></a>
        
        
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/supplementary/.pdf"><img src="bibworldfiles/supplementary.png" alt="supplementary" title="supplementary" target="_blank"/></a>
        
        
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Simon14:PDD">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Simon14:PDD.pdf">Part Detector Discovery in Deep Convolutional Neural Networks</a>.</span>
          <br />
          <span class=bibauthor>Marcel Simon and Erik Rodner and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>Asian Conference on Computer Vision (ACCV).</span>
          
          <span class=bibyear>2014.</span>
        
		
		
		<span class=note> accepted for publication</span>
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Simon14:PDD" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Simon14:PDD.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        
        <a href="https://github.com/cvjena/PartDetectorDisovery"><img src="bibworldfiles/code-link.png" alt="code" title="code" target="_blank"/></a>
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Simon14:PDD');">more ...</span>
        <div class="bibabstract" id="Simon14:PDD">
          <br />
          <div class='vspace'></div>
          Abstract: Current fine-grained classification approaches often rely  on a robust localization of object parts to extract  localized feature representations suitable for discrimination.  However, part localization is a  challenging task due to the large variation of appearance and pose.  In this paper, we show how pre-trained convolutional neural networks  can be used for robust and efficient object part discovery and localization without the  necessity to actually train the network on the current dataset. Our approach called  part detector discovery  (PDD)  is based on analyzing the gradient maps of the network outputs and finding  activation centers spatially related to annotated semantic parts or bounding boxes.  This allows us not just to obtain excellent performance on the CUB200-2011 dataset,  but in contrast to previous approaches also to perform detection and bird classification jointly  without requiring a given bounding box annotation during testing and ground-truth parts during training.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Freytag14:SIE">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Freytag14:SIE.pdf">Selecting Influential Examples: Active Learning with Expected Model  Output Changes</a>.</span>
          <br />
          <span class=bibauthor>Alexander Freytag and Erik Rodner and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>European Conference on Computer Vision (ECCV).</span>
          
          <span class=bibpages>562-577.</span>
          
          <span class=bibyear>2014.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Freytag14:SIE" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Freytag14:SIE.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/presentation/Freytag14:SIE.pdf"><img src="bibworldfiles/presentation.png" alt="presentation" title="presentation" target="_blank"/></a>
        
        
        
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Freytag14:STB">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Freytag14:STB.pdf">Seeing through bag-of-visual-word glasses: towards understanding  quantization effects in feature extraction methods</a>.</span>
          <br />
          <span class=bibauthor>Alexander Freytag and Johannes R√ºhle and Paul Bodesheim and Erik  Rodner and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>International Conference on Pattern Recognition (ICPR) - FEAST workshop.</span>
          
          <span class=bibyear>2014.</span>
        
	
		<span class=awardnote> Best Poster Award</span>
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Freytag14:STB" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Freytag14:STB.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        
        <a href="https://github.com/cvjena/bowInversion"><img src="bibworldfiles/code-link.png" alt="code" title="code" target="_blank"/></a>
        
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/presentation/Freytag14:STB.pdf"><img src="bibworldfiles/presentation.png" alt="presentation" title="presentation" target="_blank"/></a>
        
        
        
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Sickert14:SVS">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          Semantic Volume Segmentation with Iterative Context Integration.</span>
          <br />
          <span class=bibauthor>Sven Sickert and Erik Rodner and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>Open German-Russian Workshop on Pattern Recognition and Image Understanding  (OGRW).</span>
          
          <span class=bibyear>2014.</span>
        
		
		
		<span class=note> accepted for publication</span>
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Sickert14:SVS" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Sickert14:SVS');">more ...</span>
        <div class="bibabstract" id="Sickert14:SVS">
          <br />
          <div class='vspace'></div>
          Abstract: Automatic recognition of biological structures like membranes or synapses  is important to analyze organic processes and to understand their  functional behavior. To achieve this, volumetric images taken by  electron microscopy or computed tomography have to be segmented into  meaningful regions. We are extending iterative context forests which  were developed for 2D image data for image stack segmentation. In  particular, our method s able to learn high order dependencies and  import contextual information, which often can not be learned by  conventional Markov random field approaches usually used for this  task. Our method is tested for very different and challenging medical  and biological segmentation tasks.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Simon14:PLE">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Simon14:PLE.pdf">Part Localization by Exploiting Deep Convolutional Networks</a>.</span>
          <br />
          <span class=bibauthor>Marcel Simon and Erik Rodner and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>ECCV Workshop on Parts and Attributes.</span>
          
          <span class=bibyear>2014.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Simon14:PLE" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Simon14:PLE.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="https://filebox.ece.vt.edu/~parikh/PnA2014/"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Freytag14:BFF">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Freytag14:BFF.pdf">Birds of a Feather Flock Together - Local Learning of Mid-level Representations  for Fine-grained Recognition</a>.</span>
          <br />
          <span class=bibauthor>Alexander Freytag and Erik Rodner and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>ECCV Workshop on Parts and Attributes.</span>
          
          <span class=bibyear>2014.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Freytag14:BFF" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Freytag14:BFF.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="https://filebox.ece.vt.edu/~parikh/PnA2014/"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        <a href="https://github.com/cvjena/patchDiscovery"><img src="bibworldfiles/code-link.png" alt="code" title="code" target="_blank"/></a>
        
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/presentation/Freytag14:BFF.pdf"><img src="bibworldfiles/presentation.png" alt="presentation" title="presentation" target="_blank"/></a>
        
        
        
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Goering14:NPT">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Goering14:NPT.pdf">Nonparametric Part Transfer for Fine-grained Recognition</a>.</span>
          <br />
          <span class=bibauthor>Christoph G√∂ring and Erik Rodner and Alexander Freytag and Joachim  Denzler.</span>
          <br />
          <span class=bibvenue>IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</span>
          
          <span class=bibpages>2489-2496.</span>
          
          <span class=bibyear>2014.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Goering14:NPT" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Goering14:NPT.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Goring_Nonparametric_Part_Transfer_2014_CVPR_paper.pdf"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        <a href="https://github.com/cvjena/finegrained-cvpr2014"><img src="bibworldfiles/code-link.png" alt="code" title="code" target="_blank"/></a>
        
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/presentation/Goering14:NPT.pdf"><img src="bibworldfiles/presentation.png" alt="presentation" title="presentation" target="_blank"/></a>
        
        
        
        <span class="abstractlink" onClick="abstractclick('Goering14:NPT');">more ...</span>
        <div class="bibabstract" id="Goering14:NPT">
          <br />
          <div class='vspace'></div>
          Abstract: In the following paper, we present an approach for fine-grained recognition based on a new part detection method.  In particular, we propose a nonparametric label transfer technique which transfers part constellations from objects with similar global shapes.  The possibility for transferring part annotations to unseen images allows for coping with a high degree of pose and view variations in scenarios where  traditional detection models (such as deformable part models) fail.  Our approach is especially  valuable for fine-grained recognition scenarios where intraclass variations are extremely high, and  precisely localized features need to be extracted.  Furthermore, we show the importance of carefully designed visual extraction strategies, such as combination of complementary feature types and  iterative image segmentation, and the resulting impact on the recognition performance.  In experiments, our simple yet powerful approach achieves 35.9% and 57.8% accuracy on the CUB-2010 and 2011 bird datasets,  which is the current best performance for these benchmarks.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Goehring14:ITR">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Goehring14:ITR.pdf">Interactive Adaptation of Real-Time Object Detectors</a>.</span>
          <br />
          <span class=bibauthor>Daniel G√∂hring and Judy Hoffman and Erik Rodner and Kate Saenko  and Trevor Darrell.</span>
          <br />
          <span class=bibvenue>International Conference on Robotics and Automation (ICRA).</span>
          
          <span class=bibpages>1282-1289.</span>
          
          <span class=bibyear>2014.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Goehring14:ITR" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Goehring14:ITR.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="http://raptor.berkeleyvision.org/"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Goehring14:ITR');">more ...</span>
        <div class="bibabstract" id="Goehring14:ITR">
          <br />
          <div class='vspace'></div>
          Abstract: In the following paper, we present a framework for quickly training  2D object detectors for robotic perception. Our method can be used  by robotics practitioners to quickly (under 30 seconds per object)  build a large-scale real-time perception system. In particular, we  show how to create new detectors on the fly using large-scale internet  image databases, thus allowing a user to choose among thousands of  available categories to build a detection system suitable for the  particular robotic application. Furthermore, we show how to adapt  these models to the current environment with just a few in-situ images.  Experiments on existing 2D benchmarks evaluate the speed, accuracy,  and flexibility of our system.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    

</div>