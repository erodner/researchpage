<div class='block' >

    <h2>2015</h2>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Kaeding15:ALD">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Kaeding15:ALD.pdf">Active Learning and Discovery of Object Categories in the Presence of Unnameable Instances</a>.</span>
          <br />
          <span class=bibauthor>Christoph Käding and Alexander Freytag and Erik Rodner and Paul Bodesheim and Joachim  Denzler.</span>
          <br />
          <span class=bibvenue>IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</span>
          
          <span class=bibpages>4343-4352.</span>
          
          <span class=bibyear>2015.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Kaeding15:ALD" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Kaeding15:ALD.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Kading_Active_Learning_and_2015_CVPR_paper.html"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/presentation/Kaeding15:ALD.pdf"><img src="bibworldfiles/presentation.png" alt="presentation" title="presentation" target="_blank"/></a>
        
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/supplementary/.pdf"><img src="bibworldfiles/supplementary.png" alt="supplementary" title="supplementary" target="_blank"/></a>
        
        
        <span class="abstractlink" onClick="abstractclick('Kaeding15:ALD');">more ...</span>
        <div class="bibabstract" id="Kaeding15:ALD">
          <br />
          <div class='vspace'></div>
          Abstract: Current visual recognition algorithms are "hungry" for data but massive annotation is extremely costly. Therefore, active learning algorithms are required that reduce labeling efforts to a minimum by selecting examples that are most valuable for labeling. In active learning, all categories occurring in collected data are usually assumed to be known in advance and experts should be able to label every requested instance. But do these assumptions really hold in practice? Could you name all categories in every image? Existing algorithms completely ignore the fact that there are certain examples where an oracle can not provide an answer or which even do not belong to the current problem domain. Ideally, active learning techniques should be able to discover new classes and at the same time cope with queries an expert is not able or willing to label. To meet these observations, we present a variant of the expected model output change principle for active learning and discovery in the presence of unnameable instances. Our experiments show that in these realistic scenarios, our approach substantially outperforms previous active learning methods, which are often not even able to improve with respect to the baseline of random query selection.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Brust15:ECP">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Brust15:ECP.pdf">Efficient Convolutional Patch Networks for Scene Understanding</a>.</span>
          <br />
          <span class=bibauthor>Clemens-Alexander Brust and Sven Sickert and Marcel Simon and Erik Rodner and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>CVPR Workshop on Scene Understanding (CVPR-WS).</span>
          
          <span class=bibyear>2015.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Brust15:ECP" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Brust15:ECP.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        
        <a href="http://cvjena.github.io/cn24/"><img src="bibworldfiles/code-link.png" alt="code" title="code" target="_blank"/></a>
        
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/presentation/Brust15:ECP.pdf"><img src="bibworldfiles/presentation.png" alt="presentation" title="presentation" target="_blank"/></a>
        
        
        
        <span class="abstractlink" onClick="abstractclick('Brust15:ECP');">more ...</span>
        <div class="bibabstract" id="Brust15:ECP">
          <br />
          <div class='vspace'></div>
          Abstract: In this paper, we present convolutional patch networks, which are convolutional (neural) networks (CNN) learned to distinguish  different image patches and which can be used for pixel-wise labeling. We show how to easily learn spatial priors for certain categories jointly  with their appearance. Experiments for urban scene understanding demonstrate state-of-the-art results on the LabelMeFacade dataset. Our approach  is implemented as a new CNN framework especially designed for semantic segmentation with fully-convolutional architectures.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Bodesheim15:LND">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Bodesheim15:LND.pdf">Local Novelty Detection in Multi-class Recognition Problems</a>.</span>
          <br />
          <span class=bibauthor>Paul Bodesheim and Alexander Freytag and Erik Rodner and Joachim  Denzler.</span>
          <br />
          <span class=bibvenue>IEEE Winter Conference on Applications of Computer  Vision (WACV).</span>
          
          <span class=bibpages>813-820.</span>
          
          <span class=bibyear>2015.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Bodesheim15:LND" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Bodesheim15:LND.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        
        
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/supplementary/.pdf"><img src="bibworldfiles/supplementary.png" alt="supplementary" title="supplementary" target="_blank"/></a>
        
        
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Simon15:NAC">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Simon15:NAC.pdf">Neural Activation Constellations: Unsupervised Part Model Discovery with Convolutional Networks</a>.</span>
          <br />
          <span class=bibauthor>Marcel Simon and Erik Rodner.</span>
          <br />
          <span class=bibvenue>International Conference on Computer Vision (ICCV).</span>
          
          <span class=bibyear>2015.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Simon15:NAC" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Simon15:NAC.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="http://arxiv.org/abs/1504.08289"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Simon15:NAC');">more ...</span>
        <div class="bibabstract" id="Simon15:NAC">
          <br />
          <div class='vspace'></div>
          Abstract: Part models of object categories are essential for challenging recognition tasks, where differences in categories are subtle and only reflected in appearances of small parts of the object. We present an approach that is able to learn part models in a completely unsupervised manner, without part annotations and even without given bounding boxes during learning. The key idea is to find constellations of neural activation patterns computed using convolutional neural networks. In our experiments, we outperform existing approaches for fine-grained recognition on the CUB200-2011, Oxford PETS, and Oxford Flowers dataset in case no part or bounding box annotations are available and achieve state-of-the-art performance for the Stanford Dog dataset. We also show the benefits of neural constellation models as a data augmentation technique for fine-tuning. Furthermore, our paper unites the areas of generic and fine-grained classification, since our approach is suitable for both scenarios.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Dittberner15:AAC">
          
        
        </td>
        <td style='vertical-align:middle'>
        
          <span class=bibtitle>
          Automated analysis of confocal laser endomicroscopy images to detect head and neck cancer.</span>
          <br />
          <span class=bibauthor>Andreas Dittberner and Erik Rodner and Wolfgang Ortmann and Joachim Stadler and  Carsten Schmidt and Iver Petersen and Andreas Stallmach and Joachim Denzler and Orlando Guntinas-Lichius.</span>
          <br />
          <span class=bibvenue>Head and Neck.</span>
          
          
          <span class=bibyear>2015.</span>
        
		
		
		<span class=note> accepted for publication, in press</span>
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Dittberner15:AAC" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        
        
        
        
        
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Brust15:CPN">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Brust15:CPN.pdf">Convolutional Patch Networks with Spatial Prior for Road Detection and Urban Scene Understanding</a>.</span>
          <br />
          <span class=bibauthor>Clemens-Alexander Brust and Sven Sickert and Marcel Simon and Erik Rodner and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>International Conference on Computer Vision Theory and Applications (VISAPP).</span>
          
          <span class=bibpages>510-517.</span>
          
          <span class=bibyear>2015.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Brust15:CPN" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Brust15:CPN.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="http://arxiv.org/abs/1502.06344"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        <a href="http://cvjena.github.io/cn24/"><img src="bibworldfiles/code-link.png" alt="code" title="code" target="_blank"/></a>
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Brust15:CPN');">more ...</span>
        <div class="bibabstract" id="Brust15:CPN">
          <br />
          <div class='vspace'></div>
          Abstract: Classifying single image patches is important in many different applications, such as road detection or scene understanding.  In this paper, we present convolutional patch networks, which are convolutional networks learned to distinguish different image patches and  which can be used for pixel-wise labeling.  We also show how to incorporate spatial information of the patch as an input to the network, which allows for learning spatial priors for  certain categories jointly with an appearance model.  In particular, we focus on road detection and urban scene understanding, two application areas where we are able to achieve state-of-the-art  results on  the KITTI as well as on the LabelMeFacade dataset.  Furthermore, our paper offers a guideline for people working in the area and desperately wandering through all the painstaking details that  render training CNs on image patches extremely difficult.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Simon15:FCI">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Simon15:FCI.pdf">Fine-grained Classification of Identity Document Types with Only One Example</a>.</span>
          <br />
          <span class=bibauthor>Marcel Simon and Erik Rodner and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>Machine Vision Applications (MVA).</span>
          
          <span class=bibpages>126 - 129.</span>
          
          <span class=bibyear>2015.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Simon15:FCI" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Simon15:FCI.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="http://www.mva-org.jp/mva2015/FinalProgram_20150423_clean.pdf"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Simon15:FCI');">more ...</span>
        <div class="bibabstract" id="Simon15:FCI">
          <br />
          <div class='vspace'></div>
          Abstract: This paper shows how to recognize types of identity documents, such as passports, using state-of-the-art visual recognition approaches. Whereas recognizing individual parts on identity documents with a standardized layout is one of the old classics in computer vision, recognizing the type of the document and therefore also the layout is a challenging problem due to the large variation of the documents.  In our paper, we evaluate different techniques for this application including feature representations based on recent achievements with convolutional neural networks.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Ruehle15:BYC">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Ruehle15:BYC.pdf">Beyond Thinking in Common Categories: Predicting Obstacle Vulnerability using Large Random Codebooks</a>.</span>
          <br />
          <span class=bibauthor>Johannes Rühle and Erik Rodner and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>Machine Vision Applications (MVA).</span>
          
          <span class=bibpages>198-201.</span>
          
          <span class=bibyear>2015.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Ruehle15:BYC" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Ruehle15:BYC.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="http://www.mva-org.jp/mva2015/FinalProgram_20150423_clean.pdf"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Ruehle15:BYC');">more ...</span>
        <div class="bibabstract" id="Ruehle15:BYC">
          <br />
          <div class='vspace'></div>
          Abstract: Obstacle detection for advanced driver assistance systems has focused on building detectors for only a few number of categories so far, such as pedestrians and cars.  However, vulnerable obstacles of other categories are often dismissed, such as wheel-chairs and baby strollers. In our work, we try to tackle this limitation by presenting an approach which is able to predict the vulnerability of an arbitrary obstacle independently from its category. This allows for using models not specifically tuned for category recognition. To classify the vulnerability, we apply a generic category-free approach based on large random bag-of-visual-words representations (BoW), where  we make use of both the intensity image as well as a given disparity map.  In experimental results, we achieve a classification accuracy of over 80% for predicting one of four vulnerability levels for each of the 10000 obstacle hypotheses detected in a challenging dataset of real urban street scenes. Vulnerability prediction in general and our working algorithm in particular, pave the way to more advanced reasoning in autonomous driving, emergency route planning,  as well as reducing the false-positive rate of obstacle warning systems.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Rodner15:ACM">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Rodner15:ACM.pdf">Analysis and Classification of Microscopy Images with Cell Border Distance Statistics</a>.</span>
          <br />
          <span class=bibauthor>Erik Rodner and Wolfgang Ortmann and Andreas Dittberner and  Joachim Stadler and Carsten Schmidt and Iver Petersen and Andreas Stallmach  and Joachim Denzler and Orlando Guntinas-Lichius.</span>
          <br />
          <span class=bibvenue>Jahrestagung der Deutschen Gesellschaft für Medizinische Physik (DGMP).</span>
          
          <span class=bibyear>2015.</span>
        
		
		
		<span class=note> accepted for publication</span>
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Rodner15:ACM" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Rodner15:ACM.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        
        
        
        
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Guadarrama15:UOD">
          
        
        </td>
        <td style='vertical-align:middle'>
        
          <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Guadarrama15:UOD.pdf">Understanding Object Descriptions in Robotics by Open-vocabulary Object Retrieval and Detection</a>.</span>
          <br />
          <span class=bibauthor>Sergio Guadarrama and Erik Rodner and Kate Saenko and Trevor Darrell.</span>
          <br />
          <span class=bibvenue>International Journal of Robotics Research (IJRR).</span>
          
          
          <span class=bibyear>2015.</span>
        
		
		
		<span class=note> accepted for publication</span>
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Guadarrama15:UOD" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Guadarrama15:UOD.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        
        
        
        
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Rodner15:FRD">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Rodner15:FRD.pdf">Fine-grained Recognition Datasets for Biodiversity Analysis</a>.</span>
          <br />
          <span class=bibauthor>Erik Rodner and Marcel Simon and Gunnar Brehm and Stephanie Pietsch and J. Wolfgang Wägele and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>CVPR Workshop on Fine-grained Visual Classification (CVPR-WS).</span>
          
          <span class=bibyear>2015.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Rodner15:FRD" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Rodner15:FRD.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="http://www.inf-cv.uni-jena.de/fgvcbiodiv"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    

    <h2>2014</h2>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Haase14:ITL">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Haase14:ITL.pdf">Instance-weighted Transfer Learning of Active Appearance Models</a>.</span>
          <br />
          <span class=bibauthor>Daniel Haase and Erik Rodner and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</span>
          
          <span class=bibpages>1426-1433.</span>
          
          <span class=bibyear>2014.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Haase14:ITL" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Haase14:ITL.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Haase14:ITL');">more ...</span>
        <div class="bibabstract" id="Haase14:ITL">
          <br />
          <div class='vspace'></div>
          Abstract: There has been a lot of work on face modeling, analysis, and landmark  detection, with Active Appearance Models being one of the most successful  techniques. A major drawback of these models is the large number  of detailed annotated training examples needed for learning. Therefore,  we present a transfer learning method that is able to learn from  related training data using an instance-weighted transfer technique.  Our method is derived using a generalization of importance sampling  and in contrast to previous work we explicitly try to tackle the  transfer already during learning instead of adapting the fitting  process. In our studied application of face landmark detection, we  efficiently transfer facial expressions from other human individuals  and are thus able to learn a precise face Active Appearance Model  only from neutral faces of a single individual. Our approach is evaluated  on two common face datasets and outperforms previous transfer methods.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Barz14:ART">
          
        
        </td>
        <td style='vertical-align:middle'>
        
          <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Barz14:ART.pdf">ARTOS -- Adaptive Real-Time Object Detection System</a>.</span>
          <br />
          <span class=bibauthor>Björn Barz and Erik Rodner and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>arXiv preprint arXiv:1407.2721.</span>
          
          
          <span class=bibyear>2014.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Barz14:ART" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Barz14:ART.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="http://arxiv.org/abs/1407.2721"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        <a href="http://cvjena.github.io/artos/"><img src="bibworldfiles/code-link.png" alt="code" title="code" target="_blank"/></a>
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Barz14:ART');">more ...</span>
        <div class="bibabstract" id="Barz14:ART">
          <br />
          <div class='vspace'></div>
          Abstract: ARTOS is all about creating, tuning, and applying object detection  models with just a few clicks. In particular, ARTOS facilitates learning  of models for visual object detection by eliminating the burden of  having to collect and annotate a large set of positive and negative  samples manually and in addition it implements a fast learning technique  to reduce the time needed for the learning step. A clean and friendly  GUI guides the user through the process of model creation, adaptation  of learned models to different domains using in-situ images, and  object detection on both offline images and images from a video stream.  A library written in C++ provides the main functionality of ARTOS  with a C-style procedural interface, so that it can be easily integrated  with any other project.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Freytag14:BFF">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Freytag14:BFF.pdf">Birds of a Feather Flock Together - Local Learning of Mid-level Representations  for Fine-grained Recognition</a>.</span>
          <br />
          <span class=bibauthor>Alexander Freytag and Erik Rodner and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>ECCV Workshop on Parts and Attributes (ECCV-WS).</span>
          
          <span class=bibyear>2014.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Freytag14:BFF" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Freytag14:BFF.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="https://filebox.ece.vt.edu/~parikh/PnA2014/"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        <a href="https://github.com/cvjena/patchDiscovery"><img src="bibworldfiles/code-link.png" alt="code" title="code" target="_blank"/></a>
        
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/presentation/Freytag14:BFF.pdf"><img src="bibworldfiles/presentation.png" alt="presentation" title="presentation" target="_blank"/></a>
        
        
        
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Freytag14:SIE">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Freytag14:SIE.pdf">Selecting Influential Examples: Active Learning with Expected Model  Output Changes</a>.</span>
          <br />
          <span class=bibauthor>Alexander Freytag and Erik Rodner and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>European Conference on Computer Vision (ECCV).</span>
          
          <span class=bibpages>562-577.</span>
          
          <span class=bibyear>2014.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Freytag14:SIE" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Freytag14:SIE.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/presentation/Freytag14:SIE.pdf"><img src="bibworldfiles/presentation.png" alt="presentation" title="presentation" target="_blank"/></a>
        
        
        
        <span class="abstractlink" onClick="abstractclick('Freytag14:SIE');">more ...</span>
        <div class="bibabstract" id="Freytag14:SIE">
          <br />
          <div class='vspace'></div>
          Abstract: In this paper, we introduce a new general strategy for active learning.  The key idea of our approach is to measure the expected change of model outputs, a concept  that generalizes previous methods based on expected model change and incorporates the underlying data distribution.  For each example of an unlabeled set, the expected change of model predictions  is calculated and marginalized over the unknown label. This results in a score for each unlabeled example  that can be used for active learning with a broad range of models and learning algorithms.  In particular, we show how to derive very efficient active learning methods for Gaussian process regression, which  implement this general strategy, and link them to previous methods.  We analyze our algorithms and compare them to a broad range of previous active learning strategies in experiments showing that  they outperform state-of-the-art on well-established benchmark datasets in the area of visual object recognition.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Freytag14:STB">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Freytag14:STB.pdf">Seeing through bag-of-visual-word glasses: towards understanding  quantization effects in feature extraction methods</a>.</span>
          <br />
          <span class=bibauthor>Alexander Freytag and Johannes Rühle and Paul Bodesheim and Erik Rodner and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>International Conference on Pattern Recognition (ICPR) - FEAST workshop.</span>
          
          <span class=bibyear>2014.</span>
        
	
		<span class=awardnote> Best Poster Award</span>
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Freytag14:STB" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Freytag14:STB.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        
        <a href="https://github.com/cvjena/bowInversion"><img src="bibworldfiles/code-link.png" alt="code" title="code" target="_blank"/></a>
        
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/presentation/Freytag14:STB.pdf"><img src="bibworldfiles/presentation.png" alt="presentation" title="presentation" target="_blank"/></a>
        
        
        
        <span class="abstractlink" onClick="abstractclick('Freytag14:STB');">more ...</span>
        <div class="bibabstract" id="Freytag14:STB">
          <br />
          <div class='vspace'></div>
          Abstract: The bag-of-visual-word (BoW) model is one of the most common concepts for image categorization and feature extraction.  Although our community developed powerful BoW approaches for visual recognition  and it serves as a great ad-hoc solution, unfortunately, there are several drawbacks that most researchers might be not aware of.  In this paper, we aim at seeing behind the curtains and point to some of the negative aspects of these approaches which  go usually unnoticed:  (i) although BoW approaches are often motivated by relating clusters to meaningful object parts, this relation does not hold in practice with low-dimensional features such as HOG, and standard  clustering method,  (ii) clusters can be chosen randomly without loss in performance,  (iii) BoW is often only collecting background statistics, and  (iv) cluster assignments are not robust to small spatial shifts.  Furthermore, we show the effect of BoW quantization and the related loss of visual information by a simple inversion method called HoggleBoW.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Simon14:PDD">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Simon14:PDD.pdf">Part Detector Discovery in Deep Convolutional Neural Networks</a>.</span>
          <br />
          <span class=bibauthor>Marcel Simon and Erik Rodner and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>Asian Conference on Computer Vision (ACCV).</span>
          
          <span class=bibpages>162-177.</span>
          
          <span class=bibyear>2014.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Simon14:PDD" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Simon14:PDD.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        
        <a href="https://github.com/cvjena/PartDetectorDisovery"><img src="bibworldfiles/code-link.png" alt="code" title="code" target="_blank"/></a>
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Simon14:PDD');">more ...</span>
        <div class="bibabstract" id="Simon14:PDD">
          <br />
          <div class='vspace'></div>
          Abstract: Current fine-grained classification approaches often rely  on a robust localization of object parts to extract  localized feature representations suitable for discrimination.  However, part localization is a  challenging task due to the large variation of appearance and pose.  In this paper, we show how pre-trained convolutional neural networks  can be used for robust and efficient object part discovery and localization without the  necessity to actually train the network on the current dataset. Our approach called  part detector discovery  (PDD)  is based on analyzing the gradient maps of the network outputs and finding  activation centers spatially related to annotated semantic parts or bounding boxes.  This allows us not just to obtain excellent performance on the CUB200-2011 dataset,  but in contrast to previous approaches also to perform detection and bird classification jointly  without requiring a given bounding box annotation during testing and ground-truth parts during training.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Guadarrama14:OOR">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Guadarrama14:OOR.pdf">Open-vocabulary Object Retrieval</a>.</span>
          <br />
          <span class=bibauthor>Sergio Guadarrama and Erik Rodner and Kate Saenko and Ning Zhang  and Ryan Farrell and Jeff Donahue and Trevor Darrell.</span>
          <br />
          <span class=bibvenue>Robotics Science and Systems (RSS).</span>
          
          <span class=bibpages>41, ISBN 978-0-9923747-0-9.</span>
          
          <span class=bibyear>2014.</span>
        
	
		<span class=awardnote> Awarded with an AAAI invited talk</span>
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Guadarrama14:OOR" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Guadarrama14:OOR.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="http://openvoc.berkeleyvision.org"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Guadarrama14:OOR');">more ...</span>
        <div class="bibabstract" id="Guadarrama14:OOR">
          <br />
          <div class='vspace'></div>
          Abstract: In this paper, we address the problem of retrieving objects based  on open-vocabulary natural language queries: Given a phrase describing  a specific object, e.g., the corn flakes box, the task is to find  the best match in a set of images containing candidate objects. When  naming objects, humans tend to use natural language with rich semantics,  including basic-level categories, fine-grained categories, and instance-level  concepts such as brand names. Existing approaches to large-scale  object recognition fail in this scenario, as they expect queries  that map directly to a fixed set of pre-trained visual categories,  e.g. ImageNet synset tags. We address this limitation by introducing  a novel object retrieval method. Given a candidate object image,  we first map it to a set of words that are likely to describe it,  using several learned image-to-text projections. We also propose  a method for handling open-vocabularies, i.e., words not contained  in the training data. We then compare the natural language query  to the sets of words predicted for each candidate and select the  best match. Our method can combine category- and instance-level semantics  in a common representation. We present extensive experimental results  on several datasets using both instance-level and category-level  matching and show that our approach can accurately retrieve objects  based on extremely varied open-vocabulary queries. The source code  of our approach will be publicly available together with pre-trained  models and could be directly used for robotics applications.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Simon14:PLE">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Simon14:PLE.pdf">Part Localization by Exploiting Deep Convolutional Networks</a>.</span>
          <br />
          <span class=bibauthor>Marcel Simon and Erik Rodner and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>ECCV Workshop on Parts and Attributes (ECCV-WS).</span>
          
          <span class=bibyear>2014.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Simon14:PLE" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Simon14:PLE.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="https://filebox.ece.vt.edu/~parikh/PnA2014/"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Suesse14:BV">
          
        
        </td>
        <td style='vertical-align:middle'>
        
          <span class=bibtitle>Bildverarbeitung und Objekterkennung: Computer Vision in Industrie  und Medizin</span> 
          <span class=bibauthor>Herbert Süße and Erik Rodner.</span>
          (<span class=bibyear>2014</span>)
        
		
		
		<span class=note> Neues umfangreiches Lehrbuch im Bereich Bildverarbeitung und maschinelles  Lernen</span>
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Suesse14:BV" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        
        <a href="http://www.dbvbuch.de"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Suesse14:BV');">more ...</span>
        <div class="bibabstract" id="Suesse14:BV">
          <br />
          <div class='vspace'></div>
          Abstract: Dieses Buch erlaeutert, wie Informationen automatisch aus Bildern  extrahiert werden. Mit dieser sehr aktuellen Frage beschaeftigt sich  das Buch mittels eines Streifzuges durch die Bildverarbeitung. Dabei  werden sowohl die mathematischen Grundlagen vieler Verfahren der  2D- und 3D Bildanalyse vermittelt als auch deren Nutzen anhand von  Problemstellungen aus vielen Bereichen (Medizin, industrielle Bildverarbeitung,  Objekterkennung) erlaeutert. Das Buch eignet sich sowohl fuer Studierende  der Informatik, Mathematik und Ingenieurwissenschaften als auch fuer  Anwender aus der industriellen Bildverarbeitung.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Freytag14:ESP">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Freytag14:ESP.pdf">Exemplar-specific Patch Features for Fine-grained Recognition</a>.</span>
          <br />
          <span class=bibauthor>Alexander Freytag and Erik Rodner and Trevor Darrell and Joachim  Denzler.</span>
          <br />
          <span class=bibvenue>German Conference on Pattern Recognition (GCPR).</span>
          
          <span class=bibpages>144-156.</span>
          
          <span class=bibyear>2014.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Freytag14:ESP" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Freytag14:ESP.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        
        <a href="https://github.com/cvjena/patchDiscovery"><img src="bibworldfiles/code-link.png" alt="code" title="code" target="_blank"/></a>
        
        
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/supplementary/.pdf"><img src="bibworldfiles/supplementary.png" alt="supplementary" title="supplementary" target="_blank"/></a>
        
        
        <span class="abstractlink" onClick="abstractclick('Freytag14:ESP');">more ...</span>
        <div class="bibabstract" id="Freytag14:ESP">
          <br />
          <div class='vspace'></div>
          Abstract: In this paper, we present a new approach for fine-grained recognition or subordinate categorization,  tasks where an algorithm needs to reliably differentiate between visually similar categories, e.g. different bird species.  While previous approaches aim at learning a single generic representation and models with increasing complexity,  we propose an orthogonal approach that learns patch representations specifically tailored to every single test exemplar.  Since we query a constant number of images similar to a given test image,  we obtain very compact features and avoid large-scale training with all classes and examples.  Our learned mid-level features are build on shape and color detectors estimated from discovered patches reflecting small highly discriminative structures in the queried images.  We evaluate our approach for fine-grained recognition on the CUB-2011 birds dataset and show that high recognition rates can be obtained by model combination.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Hoffman14:ACI">
          
        
        </td>
        <td style='vertical-align:middle'>
        
          <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Hoffman14:ACI.pdf">Asymmetric and Category Invariant Feature Transformations for Domain  Adaptation</a>.</span>
          <br />
          <span class=bibauthor>Judy Hoffman and Erik Rodner and Jeff Donahue and Brian Kulis and  Kate Saenko.</span>
          <br />
          <span class=bibvenue>International Journal of Computer Vision (IJCV).</span>
          
          <span class=bibpages>109(1-2):</span>
          
          
          <span class=bibpages>28-41.</span>
          
          <span class=bibyear>2014.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Hoffman14:ACI" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Hoffman14:ACI.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="http://link.springer.com/article/10.1007/s11263-014-0719-3"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Hoffman14:ACI');">more ...</span>
        <div class="bibabstract" id="Hoffman14:ACI">
          <br />
          <div class='vspace'></div>
          Abstract: We address the problem of visual domain adaptation for transferring  object models from one dataset or visual domain to another. We introduce  a unified flexible model for both supervised and semi-supervised  learning that allows us to learn transformations between domains.  Additionally, we present two instantiations of the model, one for  general feature adaptation/alignment, and one specifically designed  for classification. First, we show how to extend metric learning  methods for domain adaptation, allowing for learning metrics independent  of the domain shift and the final classifier used. Furthermore, we  go beyond classical metric learning by extending the method to asymmetric,  category independent transformations. Our framework can adapt features  even when the target domain does not have any labeled examples for  some categories, and when the target and source features have different  dimensions. Finally, we develop a joint learning framework for adaptive  classifiers, which outperforms competing methods in terms of multi-class  accuracy and scalability. We demonstrate the ability of our approach  to adapt object recognition models under a variety of situations,  such as differing imaging conditions, feature types, and codebooks.  The experiments show its strong performance compared to previous  approaches and its applicability to large-scale scenarios.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Goering14:NPT">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Goering14:NPT.pdf">Nonparametric Part Transfer for Fine-grained Recognition</a>.</span>
          <br />
          <span class=bibauthor>Christoph Göring and Erik Rodner and Alexander Freytag and Joachim  Denzler.</span>
          <br />
          <span class=bibvenue>IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</span>
          
          <span class=bibpages>2489-2496.</span>
          
          <span class=bibyear>2014.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Goering14:NPT" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Goering14:NPT.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Goring_Nonparametric_Part_Transfer_2014_CVPR_paper.pdf"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        <a href="https://github.com/cvjena/finegrained-cvpr2014"><img src="bibworldfiles/code-link.png" alt="code" title="code" target="_blank"/></a>
        
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/presentation/Goering14:NPT.pdf"><img src="bibworldfiles/presentation.png" alt="presentation" title="presentation" target="_blank"/></a>
        
        
        
        <span class="abstractlink" onClick="abstractclick('Goering14:NPT');">more ...</span>
        <div class="bibabstract" id="Goering14:NPT">
          <br />
          <div class='vspace'></div>
          Abstract: In the following paper, we present an approach for fine-grained recognition based on a new part detection method.  In particular, we propose a nonparametric label transfer technique which transfers part constellations from objects with similar global shapes.  The possibility for transferring part annotations to unseen images allows for coping with a high degree of pose and view variations in scenarios where  traditional detection models (such as deformable part models) fail.  Our approach is especially  valuable for fine-grained recognition scenarios where intraclass variations are extremely high, and  precisely localized features need to be extracted.  Furthermore, we show the importance of carefully designed visual extraction strategies, such as combination of complementary feature types and  iterative image segmentation, and the resulting impact on the recognition performance.  In experiments, our simple yet powerful approach achieves 35.9% and 57.8% accuracy on the CUB-2010 and 2011 bird datasets,  which is the current best performance for these benchmarks.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Sickert14:SVS">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Sickert14:SVS.pdf">Semantic Volume Segmentation with Iterative Context Integration</a>.</span>
          <br />
          <span class=bibauthor>Sven Sickert and Erik Rodner and Joachim Denzler.</span>
          <br />
          <span class=bibvenue>Open German-Russian Workshop on Pattern Recognition and Image Understanding  (OGRW).</span>
          
          <span class=bibpages>220-225.</span>
          
          <span class=bibyear>2014.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Sickert14:SVS" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Sickert14:SVS.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="http://nbn-resolving.de/urn:nbn:de:hbz:kob7-2015051206"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Sickert14:SVS');">more ...</span>
        <div class="bibabstract" id="Sickert14:SVS">
          <br />
          <div class='vspace'></div>
          Abstract: Automatic recognition of biological structures like membranes or synapses  is important to analyze organic processes and to understand their  functional behavior. To achieve this, volumetric images taken by  electron microscopy or computed tomography have to be segmented into  meaningful regions. We are extending iterative context forests which  were developed for 2D image data for image stack segmentation. In  particular, our method s able to learn high order dependencies and  import contextual information, which often can not be learned by  conventional Markov random field approaches usually used for this  task. Our method is tested for very different and challenging medical  and biological segmentation tasks.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    
        <div class='paperblock' >
        <table>
	  
        <td width=150px>
	  
        
          
                <img width=90% class=teaserimg src="http://hera.inf-cv.uni-jena.de:6680/teaser/Goehring14:ITR">
          
        
        </td>
        <td style='vertical-align:middle'>
        
 	      <span class=bibtitle>
          <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Goehring14:ITR.pdf">Interactive Adaptation of Real-Time Object Detectors</a>.</span>
          <br />
          <span class=bibauthor>Daniel Göhring and Judy Hoffman and Erik Rodner and Kate Saenko  and Trevor Darrell.</span>
          <br />
          <span class=bibvenue>International Conference on Robotics and Automation (ICRA).</span>
          
          <span class=bibpages>1282-1289.</span>
          
          <span class=bibyear>2014.</span>
        
		
		
	
        <a href="http://hera.inf-cv.uni-jena.de:6680/bib/Goehring14:ITR" onlick="window.open(this.href,'bibtex','toolbar=no,menubar=no,status=no,height=400,width=600,resizable=yes'); return false;"><img src="bibworldfiles/get-bib-source.png" alt="BibTeX" /></a>
        
        <a href="http://hera.inf-cv.uni-jena.de:6680/pdf/Goehring14:ITR.pdf"><img src="bibworldfiles/pdf-document.png" alt="pdf" title="pdf"/></a>
        
        
        <a href="http://raptor.berkeleyvision.org/"><img src="bibworldfiles/web-link.png" alt="www" title="www" target="_blank"/></a>
        
        
        
        
        
        <span class="abstractlink" onClick="abstractclick('Goehring14:ITR');">more ...</span>
        <div class="bibabstract" id="Goehring14:ITR">
          <br />
          <div class='vspace'></div>
          Abstract: In the following paper, we present a framework for quickly training  2D object detectors for robotic perception. Our method can be used  by robotics practitioners to quickly (under 30 seconds per object)  build a large-scale real-time perception system. In particular, we  show how to create new detectors on the fly using large-scale internet  image databases, thus allowing a user to choose among thousands of  available categories to build a detection system suitable for the  particular robotic application. Furthermore, we show how to adapt  these models to the current environment with just a few in-situ images.  Experiments on existing 2D benchmarks evaluate the speed, accuracy,  and flexibility of our system.
          
        </div>
        </td>
        </table>
        </div>
        <div class='vspace'></div>
    

</div>